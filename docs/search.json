[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Experiments\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIllustration\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPalmer Penguins\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPerceptron\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLearning from Timnit Gebru (Parts 1 and 2)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/blog-1/blog1.html",
    "href": "posts/blog-1/blog1.html",
    "title": "Perceptron",
    "section": "",
    "text": "Here is the link to the python file: https://github.com/pwad1/465_blog1/blob/main/perceptron.py"
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "",
    "text": "Dr. Timnit Gebru is a computer scientist best known for her substantial work on AI ethics and algorithmic bias. She has also done substantial work on computer vision and natural language processing. She is best known for her research highlighting the biases and limitations of facial recognition technology.\nShe had co-founded the Ethical AI team at Google in 2018. In 2020, her work with Google came to an end due to a dispute regarding a paper on the limitations and biases of natural language processing models co-authored by her. Dr. Gebru claims that her paper was not accepted by Google’s journal due to it not taking AI ethics seriously. Her alleged removal from Google was widely reported in the media as an example of large technology companies silencing voices expressing concern about the nature of their operations."
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html#introduction",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html#introduction",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "",
    "text": "Dr. Timnit Gebru is a computer scientist best known for her substantial work on AI ethics and algorithmic bias. She has also done substantial work on computer vision and natural language processing. She is best known for her research highlighting the biases and limitations of facial recognition technology.\nShe had co-founded the Ethical AI team at Google in 2018. In 2020, her work with Google came to an end due to a dispute regarding a paper on the limitations and biases of natural language processing models co-authored by her. Dr. Gebru claims that her paper was not accepted by Google’s journal due to it not taking AI ethics seriously. Her alleged removal from Google was widely reported in the media as an example of large technology companies silencing voices expressing concern about the nature of their operations."
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html#notes-from-the-lecture",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html#notes-from-the-lecture",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "Notes from the lecture",
    "text": "Notes from the lecture\nDr. Gebru is particularly concerned about how facial recognition tools are used, and how they could be used, by law enforcement agencies. Firstly, facial recognition models face data related constraints, especially among racial and ethnic minorities, and individuals that are non cisgender men. These constrains can result in poorer recognition of these minorities, putting these individuals at greater risk of being interrogated or imprisoned without cause. These tools are also less effective in recognizing cultural differences in non-white cultures, due to biases in data.\nShe also argues that the practices used by law enforcement agencies to collect data for these purposes is highly suspect. States, such as Maryland, use extensive level surveillance programs. Law enforcement agencies have also scraped social media sites to find pictures of individuals participating in certain political activities, and have used their facial recognition models to identify these individuals. According to Dr. Gebru, these practices stifle political dissent and limit personal freedom.\nAdditionally, facial recognition tools might not be used by law enforcement agencies in the ways in which these tools were intended to be used. Hence, biases that are rampant in these institutions could be magnified by these technologies. These tools can also be used by authoritarian regimes to maintain control and establish a police state.\nTl;dr: computer vision technologies are expected to magnify biases that already exist within our societies. To prevent this from happening, we need to be more conscious about how diverse our data is, and how are data is being collected. We also need to be conscious about biases within our models and have a sense of who it is benefiting and who it harming."
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html#proposed-question",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html#proposed-question",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "Proposed Question:",
    "text": "Proposed Question:\nThe advancement of computer vision and natural langauge processing models has some obvious benefits and drawbacks. Dr. Gebru’s work does a great job in identifying and studying manners in these models may magnify systemic biases experienced by racial minorities and individuals that are not cisgender men. She is also very convincing in her arguments about how authoritarian regimes may misuse these technologies. However, just as building a model may put certain people at risk, not building a model may also prevent a group of people from realizing certain benefits. How do we assess the opportunity cost of unrealized benefits with the risks posed by machine learning models within the context of ML-related regulation?"
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html#reflection-on-dr.-gebrus-class-discussion",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html#reflection-on-dr.-gebrus-class-discussion",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "Reflection on Dr. Gebru’s class discussion",
    "text": "Reflection on Dr. Gebru’s class discussion\nI agree with Dr. Gebru’s idea that funding for ML-related research needs to be more diversified. When most of the funding comes from the government and private corporations with certain vested interests, it is inevitable that these interests, which prioritize what is beneficial at the short term over what is beneficial over the long term.\nHowever, I am not sure if I am as pessimistic about the future of ML as Dr Gebru. In our conversation, it seemed as though Dr. Gebru was discounting the benefits that ML has brought, and can bring, to vulnerable communities.\nI really enjoyed Xianzhi’s question about the construction of an alternative reality in which everything computer vision can be rebuilt in a manner that is beneficial for society at large in the long term. I found it somewhat surprising that, according to Dr. Gebru, in an ideal society, computer vision should not exist. It felt as though Professor Gebru was ignoring some of positive effects of computer vision. For example, it results in improved medical diagnostics and helps make the internet more accesible. Dr. Gebru, due to her years of experience in this field, may have a greater degree of pessimism associated with the effects of machine learning that might be unfathomable to me.\nI was also slightly disappointed by her response to my question. My question focused on the weighing the opportunity costs associated with putting restrictions on AI research. The idea behind this question is that while the research and applications of machine learning technologies are doing a lot of bad in the world, they are also doing a lot of good. In my opinion, good that is prevented from happening is equivalent to something bad being done. I was expecting an answer that weighed the bad done by ML against the good. However, I do not think that her response to my question did this. I think that she made an argument related to path dependency which somewhat satisfied me."
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html#dr.-gebrus-presentation-at-hillcrest",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html#dr.-gebrus-presentation-at-hillcrest",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "Dr. Gebru’s presentation at Hillcrest",
    "text": "Dr. Gebru’s presentation at Hillcrest\nDr. Gebru’s discussed several intellectual movements- denoted by the umbrella term TESCREAL- that are currently involved in the study of AI ethics and AI risk minization, and argued that these movements are tied to eugenics. She suggested that some of the personalities, such as Eliezer Yudkowsky, involved in these movements are racist, problematic and poorly equipped to handle this topic. She also argues that the God Complex harbored by some in this field is problematic. She also discussed her disdain for the term “AGI.” She argued that this term is extremely vague and that it has effectively become a buzzword for companies such as OpenAI. She also expressed her disdain for for certain tech-evangelists, like Sam Altman who believe that machine learning will change the world for the better. She touched upon the problematic history of statistics, and how it was tied in with the development of Eugenics. In fact, Galton, a pioneering figure in modern statistics, is also one of the foremeost figures in the Eugenics movement."
  },
  {
    "objectID": "posts/blog-timnit-gebru/TimnitGebru.html#reflection-on-dr.-gebrus-presentation-at-hillcrest",
    "href": "posts/blog-timnit-gebru/TimnitGebru.html#reflection-on-dr.-gebrus-presentation-at-hillcrest",
    "title": "Learning from Timnit Gebru (Parts 1 and 2)",
    "section": "Reflection on Dr. Gebru’s presentation at Hillcrest",
    "text": "Reflection on Dr. Gebru’s presentation at Hillcrest\nI was really looking forward to Dr. Gebru’s talk- I really enjoyed the lecture Dr. Chodrow had posted to on the assignments page- and am also familiar with some of her other work. I’ve enjoyed listening to her one of my favorite podcasts- In Machines We Trust- and really liked her paper on stochastic parrots. I was, however, somewhat disappointed by her talk. I am familiar with the ideas espoused by Effective Altruism. I do not agree with these ideas. I think that Effective Altruism’s mission is deeply flawed- the idea that a select few wealthy individuals should leverege their wealth to change the world in a manner that they deem fit is undemocratic and dangerous. I also believe that this movement is used by corporations, such as FTX, to put on a facade of doing good whilst obscuring their unethical deeds. However, I think that it is a stretch to dub this movement to be eugenicist.\nI am not too familiar with the other movements she mentioned- such as transhumanism, extropianism and singulartarianism- however I do not think that she effectively showed how any of the TESCREAL movements are definitionally eugenics. I think that the question that Tim asked in the question and answer session was particularly important. It is important to define what eugenics is to show that certain movements are eugenicist. Her response to that- which was a version of “go read a book”- felt inappropriate. An earnest answer to that question was central to the discussion- and hence there was value lost in this interaction.\nIt also felt as though she was cherry picking quotes from cherry picked figures in these movements- and she used sweeping generalizations to argue that these movements are eugenicist. Overall, I was a little disappointed in the lack of nuance in her argument. This could be a result of time constraints. I was really looking forward to this session with Dr. Gebru, am a little disheartened by how it turned out."
  },
  {
    "objectID": "posts/blog-1/blog1.html#experiment-1",
    "href": "posts/blog-1/blog1.html#experiment-1",
    "title": "Perceptron",
    "section": "Experiment 1",
    "text": "Experiment 1\nUsing 2d data like the data in the example, if the data is linearly separable then the perceptron algorithm converges to weight vector w describing a separating line (provided that the maximum number of iterations is large enough).\nPlease show visualizations of the data, the separating line, and the evolution of the accuracy over training. It’s also fine for you to use the loss instead of the accuracy if you’d prefer.\n(The two visualizations above show the data with the line of separation and the evolution of the accuracy over time)\nAs we can observe in the first chart, the data is linearly separable, and the line of separation, as produced by the Perceptron, separates the two colors.\nThe second chart indicates the accuracy for the perceptron, based on the proportion of points classified correctly, by the number of iterations.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\np.w\nprint(p.history[-10:])\n\nw= np.array([1, 2, 3, 4, 5])\nx= np.array([2, 2, 3, 4, 5])\nb= 1000\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#print(Perceptron.perceptron_classify(w, b ,x))\n\n#print(Perceptron.fit(w,x))\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/blog-1/blog1.html#experiment-2",
    "href": "posts/blog-1/blog1.html#experiment-2",
    "title": "Perceptron",
    "section": "Experiment 2",
    "text": "Experiment 2\nFor 2d data, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of w, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\nPlease show visualizations of the data, the line in the final iteration, and the evolution of the score over training.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers =  [(0, 0), (0, 0)])\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\np.w\nprint(p.history[-10:])\n\nw= np.array([1, 2, 3, 4, 5])\nx= np.array([2, 2, 3, 4, 5])\nb= 1000\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n[0.52, 0.54, 0.45, 0.48, 0.48, 0.41, 0.47, 0.48, 0.48, 0.48]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe perceptron algorithm is also able to work in more than 2 dimensions! Show an example of running your algorithm on data with at least 5 features. You don’t need to visualize the data or the separating line, but you should still show the evolution of the score over the training period. Include a comment on whether you believe that the data is linearly separable based on your observation of the score.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(8, 8), (-8, -8), (0, 0), (4, 4)])\n\np = Perceptron()\np.fit(X, y, max_steps=50000)\np.w\nprint(p.history[-10:])\n\nw= np.array([1, 2, 3, 4, 5])\nx= np.array([2, 2, 3, 4, 5])\nb= 1000\n\n[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n\n\nHere is the link to the python file: https://github.com/pwad1/465_blog1/blob/main/perceptron.py"
  },
  {
    "objectID": "posts/blog-2/GradientDescent.html",
    "href": "posts/blog-2/GradientDescent.html",
    "title": "Illustration",
    "section": "",
    "text": "from solutions import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nLR.fit_stochastic(X, y, max_epochs = 1000, batch_size=10, alpha=0.1)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# inspect the fitted value of w\nLR.w \nprint(LR.w)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nfig = draw_line(LR.w, -2, 2)\n\n[ 1.47117134  1.46657887 -0.057082  ]\n\n\n\n\n\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nnum_steps = len(LR.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history_st, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nxlab = plt.xlabel(\"Number of iterations\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nIn the above plot, we can see that the curve representing stoachastic converges to the minimum loss much faster than the curve representing the gradient. This suggests that our algorithm for stochastic descent is significantly more efficient.\n\nExperiments\n\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nLR_e1 = LogisticRegression()\nLR_e1.fit(X, y, alpha = 100, max_epochs = 1000)\n\n\n# inspect the fitted value of w\nLR_e1.w \nprint(LR_e1.w)\nfig2 = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfig2 = draw_line(LR_e1.w, -2, 2)\n\n[ 29.9437262   24.67590194 -10.61155879]\n\n\n\n\n\n\nnum_steps = len(LR_e1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_e1.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nxlab = plt.xlabel(\"Number of iterations\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs the above loss history chart shows, with a sufficiently high learning rate, which, in our case is 100, our algorithm is no longer able to converge to the minimum loss. A converging loss function would always have a negative slope. The curve (curves?) above have positive as well as negative slopes, suggesting that the loss decreases and increrases with increasing iterations.\n\n\nA case in which the choice of batch size influences how quickly the algorithm converges.\n\nLR_e2i = LogisticRegression()\nLR_e2ii = LogisticRegression()\nLR_e2iii = LogisticRegression()\nLR_e2iv = LogisticRegression()\nLR_e2v = LogisticRegression()\nLR_e2vi = LogisticRegression()\nLR_e2vii = LogisticRegression()\n\n\nLR_e2i.fit_stochastic(X, y, max_epochs = 1000, batch_size=2, alpha=0.1)\nLR_e2ii.fit_stochastic(X, y, max_epochs = 1000, batch_size=3, alpha=0.1)\nLR_e2iii.fit_stochastic(X, y, max_epochs = 1000, batch_size=5, alpha=0.1)\nLR_e2iv.fit_stochastic(X, y, max_epochs = 1000, batch_size=10, alpha=0.1)\nLR_e2v.fit_stochastic(X, y, max_epochs = 1000, batch_size=20, alpha=0.1)\nLR_e2vi.fit_stochastic(X, y, max_epochs = 1000, batch_size=40, alpha=0.1)\nLR_e2vii.fit_stochastic(X, y, max_epochs = 1000, batch_size=80, alpha=0.1)\n\n# inspect the fitted value of w\nLR_e2i.w \nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nnum_steps = len(LR_e2vii.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2vii.loss_history_st, label = \"batch size=80\")\n\nnum_steps = len(LR_e2vi.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2vi.loss_history_st, label = \"batch size=40\")\n\nnum_steps = len(LR_e2v.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2v.loss_history_st, label = \"batch size=20\")\n\nnum_steps = len(LR_e2iv.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2iv.loss_history_st, label = \"batch size=10\")\n\nnum_steps = len(LR_e2iii.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2iii.loss_history_st, label = \"batch size=5\")\n\nnum_steps = len(LR_e2ii.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2ii.loss_history_st, label = \"batch size=3\")\n\nnum_steps = len(LR_e2i.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR_e2i.loss_history_st, label = \"batch size=2\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe observe a significant difference in the loss history for different batch sizes. We can see that smaller batch sizes tend to result in convergence to the minimum loss at a faster rate. However, for extremely small batch sizes, such as batch sizes of 2 or 3, the convergence is not completely uniform.\n\n\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large. In at least one of these experiments, generate some synthetic data (it’s fine to use make_blobs) for data of at least 10 feature dimensions.\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nLR2= LogisticRegression()\nLR2.fit(X, y, alpha = 100, max_epochs = 1000)\n\n\n\n\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"LR=0.1\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"LR=100\")\n#plt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nAs we can observe, our inference about the impact of a sufficiently high learning rate also holds true for examples with 10 features. The blue curve, representing a learning rate of 0.1 uniformly converges to the minimum loss for our regression. The orange curve does not converge to any value, and fluctuates with increasing and decreasing loss values for all iterations."
  },
  {
    "objectID": "posts/blog-2/GradientDescent.html#illustration",
    "href": "posts/blog-2/GradientDescent.html#illustration",
    "title": "Optimization for Logistic Regression",
    "section": "Illustration",
    "text": "Illustration\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nnum_steps = len(LR.loss_history_st)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history_st, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/blog-3/blog3.html",
    "href": "posts/blog-3/blog3.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Here is the link to the python file:\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n27\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN46A1\nYes\n11/29/07\n44.5\n14.3\n216.0\n4100.0\nNaN\n7.96621\n-25.69327\nNaN\n\n\n1\nPAL0708\n22\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN41A2\nYes\n11/27/07\n45.1\n14.5\n215.0\n5000.0\nFEMALE\n7.63220\n-25.46569\nNaN\n\n\n2\nPAL0910\n124\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN67A2\nYes\n11/16/09\n41.4\n18.5\n202.0\n3875.0\nMALE\n9.59462\n-25.42621\nNaN\n\n\n3\nPAL0910\n146\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN82A2\nYes\n11/16/09\n39.0\n18.7\n185.0\n3650.0\nMALE\n9.22033\n-26.03442\nNaN\n\n\n4\nPAL0708\n24\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nNo\n11/28/07\n50.6\n19.4\n193.0\n3800.0\nMALE\n9.28153\n-24.97134\nNaN\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n7.63220\n-25.46569\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n9.59462\n-25.42621\n0\n0\n1\n1\n0\n1\n0\n1\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n9.22033\n-26.03442\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n9.28153\n-24.97134\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n9.04218\n-26.15775\n0\n1\n0\n1\n0\n1\n1\n0"
  },
  {
    "objectID": "posts/blog-3/blog3.html#feature-selection",
    "href": "posts/blog-3/blog3.html#feature-selection",
    "title": "Palmer Penguins",
    "section": "Feature selection",
    "text": "Feature selection\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\") #I got a whole load of warnings making my notebook look ugly.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Island\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\",\"Delta 13 C (o/oo)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    score=LR.score(X_train[cols], y_train)\n    if score==1:\n        print(cols, score)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 1.0\n\n\nIt looks like we have three combinations giving us 100% accuracy! Let’s test one of them again.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nscore=LR.score(X_train[cols], y_train)\nprint(score)\n\nwarnings.resetwarnings() #warnings are still useful! I am reactivating warnings.\n\n1.0\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nX_train[cols]\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n1\n1\n0\n0\n45.1\n14.5\n\n\n2\n0\n0\n1\n41.4\n18.5\n\n\n3\n0\n1\n0\n39.0\n18.7\n\n\n4\n0\n1\n0\n50.6\n19.4\n\n\n5\n0\n1\n0\n33.1\n16.1\n\n\n...\n...\n...\n...\n...\n...\n\n\n269\n0\n1\n0\n41.1\n17.5\n\n\n270\n1\n0\n0\n45.4\n14.6\n\n\n271\n0\n0\n1\n36.2\n17.2\n\n\n272\n1\n0\n0\n50.0\n15.9\n\n\n273\n1\n0\n0\n48.2\n14.3\n\n\n\n\n256 rows × 5 columns\n\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[cols], y_train)"
  }
]