{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745fb38c-5b9d-49d7-8e91-f6deb18bedfe",
   "metadata": {},
   "source": [
    "# Learning from Timnit Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2b692-076a-4760-8970-501ccc508881",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Dr. Timnit Gebru is a computer scientist best known for her substantial work on AI ethics and algorithmic bias. She has also done substantial work on computer vision and natural language processing. She is best known for her research highlighting the biases and limitations of facial recognition technology.\n",
    "\n",
    "She had co-founded the Ethical AI team at Google in 2018. In 2020, her work with Google came to an end due to a dispute regarding a paper on the limitations and biases of natural language processing models co-authored by her. Dr. Gebru claims that her paper was not accepted by Google's journal due to it not taking AI ethics seriously. Her alleged removal from Google was widely reported in the media as an example of large technology companies silencing voices expressing concern about the nature of their operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f249e-cc8c-4bcc-b955-228f75340914",
   "metadata": {},
   "source": [
    "# Notes from the lecture\n",
    "\n",
    "Dr. Gebru is particularly concerned about how facial recognition tools are used, and how they could be used, by law enforcement agencies. Firstly, facial recognition models face data related constraints, especially among racial and ethnic minorities, and individuals that are non cisgender men. These constrains can result in poorer recognition of these minorities, putting these individuals at greater risk of being interrogated or imprisoned without cause. These tools are also less effective in recognizing cultural differences in non-white cultures, due to biases in data.\n",
    "\n",
    "She also argues that the practices used by law enforcement agencies to collect data for these purposes is highly suspect. States, such as Maryland, use extensive level surveillance programs. Law enforcement agencies have also scraped social media sites to find pictures of individuals participating in certain political activities, and have used their facial recognition models to identify these individuals. According to Dr. Gebru, these practices stifle political dissent and limit personal freedom.  \n",
    "\n",
    "Additionally, facial recognition tools might not be used by law enforcement agencies in the ways in which these tools were intended to be used. Hence, biases that are rampant in these institutions could be magnified by these technologies. These tools can also be used by authoritarian regimes to maintain control and establish a police state.\n",
    "\n",
    "Tl;dr: computer vision technologies are expected to magnify biases that already exist within our societies. To prevent this from happening, we need to be more conscious about how diverse our data is, and how are data is being collected. We also need to be conscious about biases within our models and have a sense of who it is benefiting and who it harming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f6243-96c4-4809-84c3-68151e2344c6",
   "metadata": {},
   "source": [
    "# Proposed Question:\n",
    "\n",
    "The advancement of computer vision and natural langauge processing models has some obvious benefits and drawbacks. Dr. Gebru's work does a great job in identifying and studying manners in these models may magnify systemic biases experienced by racial minorities and individuals that are not cisgender men. She is also very convincing in her arguments about how authoritarian regimes may misuse these technologies. However, just as building a model may put certain people at risk, not building a model may also prevent a group of people from realizing certain benefits. How do we assess the opportunity cost of unrealized benefits with the risks posed by machine learning models within the context of ML-related regulation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d410e27-b583-4d67-992e-aeb5319bd4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451]",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
